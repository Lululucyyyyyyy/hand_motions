{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from cnn_utils import *\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(169, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "#directions = ['up', 'down', 'left', 'right', 'zoom_in', 'zoom_out', 'two', 'three', 'four', 'five']\n",
    "my_list = []\n",
    "path = \"dataset/*.JPG\"\n",
    "temp = glob.glob(path)\n",
    "for image in temp:\n",
    "    with open(image, 'rb') as file:\n",
    "        img = Image.open(file)\n",
    "        img = img.resize((224, 224))\n",
    "        np_img = np.array(img)\n",
    "        my_list.append(np_img)\n",
    "my_list = np.array(my_list)\n",
    "print(my_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "# 0 up, 1 down, 2 left, 3 right, 4 zoom in, 5 zoom out, 6 two, 7 three, 8 four, 9 five\n",
    "labels = [1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1, #16\n",
    "          9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9,9, #17\n",
    "          8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8, #17\n",
    "          4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4, #17\n",
    "          2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2, #17\n",
    "          5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5, #17\n",
    "          3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3, #17\n",
    "          7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7, #16\n",
    "          6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,#16\n",
    "          0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0, #19\n",
    "         ]\n",
    "print(len(labels))\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.c_[my_list.reshape(len(my_list), -1), labels.reshape(len(labels), -1)]\n",
    "X_orig = c[:, :my_list.size//len(my_list)].reshape(my_list.shape)\n",
    "Y_orig = c[:, my_list.size//len(my_list):].reshape(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n",
      "146\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "X_test_orig = X_orig[0:10]\n",
    "Y_test_orig = Y_orig[0:10]\n",
    "X_dev_orig = X_orig[12:22]\n",
    "Y_dev_orig = Y_orig[12:22]\n",
    "X_train_orig = X_orig[23:169]\n",
    "Y_train_orig = Y_orig[23:169]\n",
    "print(len(X_train_orig))\n",
    "print(len(Y_train_orig))\n",
    "print(len(X_dev_orig))\n",
    "print(len(Y_dev_orig))\n",
    "print(len(X_test_orig))\n",
    "print(len(Y_test_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train_orig/255\n",
    "X_test = X_test_orig/255\n",
    "X_dev = X_dev_orig/225\n",
    "Y_train = convert_to_one_hot(Y_train_orig, 10).T\n",
    "Y_test = convert_to_one_hot(Y_test_orig, 10).T\n",
    "Y_dev= convert_to_one_hot(Y_dev_orig, 10).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    X = tf.placeholder(tf.float32, [None, n_H0, n_W0, n_C0])\n",
    "    Y = tf.placeholder(tf.float32, [None, n_y])\n",
    "    return X, Y\n",
    "\n",
    "def initialize_parameters():\n",
    "    tf.set_random_seed(1)\n",
    "    W1 = tf.get_variable(\"W1\", [4, 4, 3, 8], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    W2 = tf.get_variable(\"W2\", [2, 2, 8, 16], initializer = tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    parameters = {\"W1\": W1,\"W2\": W2}\n",
    "    return parameters\n",
    "\n",
    "def forward_prop(X, parameters):\n",
    "    W1 = parameters['W1']\n",
    "    W2 = parameters['W2']\n",
    "    Z1 = tf.nn.conv2d(X,W1, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    P1 = tf.nn.max_pool(A1, ksize = [1,8,8,1], strides = [1,8,8,1], padding = 'SAME')\n",
    "    Z2 = tf.nn.conv2d(P1,W2, strides = [1,1,1,1], padding = 'SAME')\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    P2 = tf.nn.max_pool(A2, ksize = [1,4,4,1], strides = [1,4,4,1], padding = 'SAME')\n",
    "    P2 = tf.layers.Flatten()(P2)\n",
    "    Z3 = tf.keras.layers.Dense(10, activation=None)(P2)\n",
    "    return Z3\n",
    "\n",
    "def compute_cost(Z3, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=Z3,labels=Y))\n",
    "    #cost = -tf.reduce_sum(Y*tf.log(tf.clip_by_value(Z3,1e-10,1.0)))\n",
    "    #cost = -tf.reduce_sum(Y*tf.log(tf.clip_by_value(Z3,1e-10,1.0)))\n",
    "    #labels_sum = tf.reduce_sum(Y, axis=-1)\n",
    "    #print(labels)\n",
    "    #softmax = tf.nn.softmax(Z3)\n",
    "    #cost = tf.reduce_mean(-tf.reduce_sum(softmax * tf.log(Y), axis=-1))\n",
    "    return cost\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009, num_epochs = 100, minibatch_size = 64, print_cost = True):\n",
    "    ops.reset_default_graph()                         \n",
    "    tf.set_random_seed(1)\n",
    "    seed = 3                                          \n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = [] \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "    parameters = initialize_parameters()\n",
    "    Z3 = forward_prop(X, parameters)\n",
    "    cost = compute_cost(Z3, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            minibatch_cost = 0\n",
    "            num_minibatches = int(m / minibatch_size)\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _ , temp_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y})\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            if print_cost == True and epoch % 1 == 0:\n",
    "                costs.append(minibatch_cost)\n",
    "  \n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "        plt.show()\n",
    "\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        \n",
    "        #saver.save(sess, 'my_test_model',global_step = 50)\n",
    "\n",
    "        #return train_accuracy, parameters\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        np.random.seed(1)\n",
    "        (m, n_H0, n_W0, n_C0) = X_.shape             \n",
    "        n_y = Y_.shape[1]  \n",
    "        X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y)\n",
    "        Z3 = forward_prop(X, parameters)\n",
    "        cost = compute_cost(Z3, Y)\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        a = sess.run(cost, {X: X_, Y:Y_})\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(accuracy)\n",
    "        dev_accuracy = accuracy.eval({X: X_, Y: Y_})\n",
    "        \n",
    "    return train_accuracy, dev_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/lululucyyyyyyy/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-18-d7ed3c904312>:27: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Cost after epoch 0: 3.593359\n",
      "Cost after epoch 5: 3.191627\n",
      "Cost after epoch 10: 2.787338\n",
      "Cost after epoch 15: 2.129671\n",
      "Cost after epoch 20: 1.713653\n",
      "Cost after epoch 25: 1.308870\n",
      "Cost after epoch 30: 0.920398\n",
      "Cost after epoch 35: 0.471667\n",
      "Cost after epoch 40: 0.248950\n",
      "Cost after epoch 45: 0.169189\n",
      "Cost after epoch 50: 0.081160\n",
      "Cost after epoch 55: 0.046735\n",
      "Cost after epoch 60: 0.025605\n",
      "Cost after epoch 65: 0.018979\n",
      "Cost after epoch 70: 0.017533\n",
      "Cost after epoch 75: 0.013490\n",
      "Cost after epoch 80: 0.011417\n",
      "Cost after epoch 85: 0.010112\n",
      "Cost after epoch 90: 0.009065\n",
      "Cost after epoch 95: 0.007539\n"
     ]
    }
   ],
   "source": [
    "train, dev, parameters = model(X_train, Y_train, X_dev, Y_dev)\n",
    "print(train, dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
